{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd4e4dc1",
   "metadata": {},
   "source": [
    "# Model Building & Evaluation\n",
    "\n",
    "**Objective**:  \n",
    "Train and evaluate fraud detection models using proper preprocessing, imbalance handling, and metrics.\n",
    "\n",
    "This notebook uses modular functions from `src/model_preprocessing.py`.\n",
    "___\n",
    "## 1. Setup & Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a72959a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Add project root (one directory above \"notebooks\")\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1fbfe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.model_preprocessing import prepare_data_for_modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33f60f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraud_Data loaded: (151112, 13)\n",
      "CreditCard loaded: (283726, 31)\n"
     ]
    }
   ],
   "source": [
    "# Load engineered datasets\n",
    "fraud_df = pd.read_csv('../data/processed/fraud_data_engineered.csv')\n",
    "cc_df = pd.read_csv('../data/processed/creditcard_processed.csv')\n",
    "print(\"Fraud_Data loaded:\", fraud_df.shape)\n",
    "print(\"CreditCard loaded:\", cc_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983fb322",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Data Transformation & Handling Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90edf06b",
   "metadata": {},
   "source": [
    "a. `fraud_data_engineered.csv` \n",
    "\n",
    "\n",
    "- **justification** : For the E-commerce dataset (9.4% fraud), I chose SMOTE over undersampling. Undersampling would have required discarding over 80% of the legitimate transaction data, significantly reducing the model's ability to learn normal patterns. Since the minority class was sufficiently represented (not extremely rare), SMOTE allowed me to balance the classes while retaining all valuable information from the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "482f089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_df# Separate features and target\n",
    "X_fraud = fraud_df.drop('class', axis=1)\n",
    "y_fraud = fraud_df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2deb1547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Preparing Fraud_Data for Modeling ===\n",
      "Before Handling imbalance:\n",
      "Train: (120889, 12), Test: (30223, 12)\n",
      "Fitting preprocessor on training data...\n",
      "Applying SMOTE...\n",
      "Class distribution BEFORE balancing:\n",
      "{0: 0.9064, 1: 0.0936}\n",
      "Class distribution AFTER balancing:\n",
      "{0: 0.5, 1: 0.5}\n",
      "✅ Ready for modeling! Train Shape: (219136, 197)\n"
     ]
    }
   ],
   "source": [
    "# Split and balance — SMOTE is good here (~9.4% fraud)\n",
    "X_train_bal, y_train_bal, X_test_proc, y_test, preprocessor = prepare_data_for_modeling(\n",
    "    X_fraud, y_fraud,\n",
    "    dataset_name=\"Fraud_Data\",\n",
    "    imbalance_technique=\"smote\",   # Change to \"undersample\" for creditcard\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3022bb0b",
   "metadata": {},
   "source": [
    "a. `creditcard_processed.csv` \n",
    "\n",
    "\n",
    "- **justification** : The class imbalance in the Credit Card dataset is extreme (0.17% fraud vs 99.83% legitimate). While 1:1 Undersampling was considered, it was rejected because it would require discarding over 99% of the legitimate transactions (reducing ~227,000 rows to just ~756). This massive information loss would prevent the model from learning complex \"normal\" behaviors, leading to high False Positive rates.\n",
    "Instead, I opted for `SMOTETomek`. This hybrid technique offers the best of both worlds: it uses SMOTE to upsample the minority class (allowing us to retain the rich information in the majority class) and subsequently applies Tomek Links to remove noisy/overlapping data points at the decision boundary. This creates a clearer separation between fraud and legitimate transactions without the noise usually introduced by pure SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fa11ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Preparing CreditCard for Modeling ===\n",
      "Before Handling imbalance:\n",
      "Train: (226980, 30), Test: (56746, 30)\n",
      "Fitting preprocessor on training data...\n",
      "Applying SMOTETOMEK...\n",
      "Class distribution BEFORE balancing:\n",
      "{0: 0.9983, 1: 0.0017}\n",
      "Class distribution AFTER balancing:\n",
      "{0: 0.5, 1: 0.5}\n",
      "✅ Ready for modeling! Train Shape: (453204, 30)\n"
     ]
    }
   ],
   "source": [
    "X_cc = cc_df.drop('Class', axis=1)\n",
    "y_cc = cc_df['Class']\n",
    "X_train_bal, y_train_bal, X_test_proc, y_test, preprocessor = prepare_data_for_modeling(\n",
    "    X_cc, y_cc,\n",
    "    dataset_name=\"CreditCard\",\n",
    "    imbalance_technique=\"smotetomek\",  # or \"smotetomek\"\n",
    "    test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c032d31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
