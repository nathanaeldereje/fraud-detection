{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd4e4dc1",
   "metadata": {},
   "source": [
    "# Model Building & Evaluation\n",
    "\n",
    "**Objective**:  \n",
    "Train and evaluate fraud detection models using proper preprocessing, imbalance handling, and metrics.\n",
    "\n",
    "This notebook uses modular functions from `src/model_preprocessing.py`.\n",
    "___\n",
    "## 1. Setup & Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a72959a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Add project root (one directory above \"notebooks\")\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1fbfe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.model_preprocessing import prepare_data_for_modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33f60f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraud_Data loaded: (151112, 13)\n",
      "CreditCard loaded: (283726, 31)\n"
     ]
    }
   ],
   "source": [
    "# Load engineered datasets\n",
    "fraud_df = pd.read_csv('../data/processed/fraud_data_engineered.csv')\n",
    "cc_df = pd.read_csv('../data/processed/creditcard_processed.csv')\n",
    "print(\"Fraud_Data loaded:\", fraud_df.shape)\n",
    "print(\"CreditCard loaded:\", cc_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983fb322",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Data Transformation & Handling Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90edf06b",
   "metadata": {},
   "source": [
    "a. `fraud_data_engineered.csv` \n",
    "\n",
    "\n",
    "- **justification** : For the E-commerce dataset (9.4% fraud), I chose SMOTE over undersampling. Undersampling would have required discarding over 80% of the legitimate transaction data, significantly reducing the model's ability to learn normal patterns. Since the minority class was sufficiently represented (not extremely rare), SMOTE allowed me to balance the classes while retaining all valuable information from the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "482f089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_df# Separate features and target\n",
    "X_fraud = fraud_df.drop('class', axis=1)\n",
    "y_fraud = fraud_df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2deb1547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Preparing Fraud_Data for Modeling ===\n",
      "Before Handling imbalance:\n",
      "Train: (120889, 12), Test: (30223, 12)\n",
      "Fitting preprocessor on training data...\n",
      "Applying SMOTE...\n",
      "Class distribution BEFORE balancing:\n",
      "{0: 0.9064, 1: 0.0936}\n",
      "Class distribution AFTER balancing:\n",
      "{0: 0.5, 1: 0.5}\n",
      "✅ Ready for modeling! Train Shape: (219136, 197)\n"
     ]
    }
   ],
   "source": [
    "# Split and balance — SMOTE is good here (~9.4% fraud)\n",
    "X_train_bal, y_train_bal, X_test_proc, y_test, preprocessor = prepare_data_for_modeling(\n",
    "    X_fraud, y_fraud,\n",
    "    dataset_name=\"Fraud_Data\",\n",
    "    imbalance_technique=\"smote\",   # Change to \"undersample\" for creditcard\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3022bb0b",
   "metadata": {},
   "source": [
    "a. `creditcard_processed.csv` \n",
    "\n",
    "\n",
    "- **justification** : For the Credit Card dataset, the class imbalance was extreme (0.17% fraud vs 99.83% legitimate). Unlike the E-commerce data, applying standard SMOTE here would have required generating approximately 600 synthetic samples for every real fraud instance. This would have introduced massive synthetic noise, causing the model to learn the mathematical artifacts of SMOTE rather than real fraud patterns. Therefore, I opted for Undersampling. This approach reduced the overwhelming volume of the majority class to create a balanced training set, allowing the model to identify the decision boundary clearly without the risk of overfitting on synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fa11ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Preparing CreditCard for Modeling ===\n",
      "Before Handling imbalance:\n",
      "Train: (226980, 30), Test: (56746, 30)\n",
      "Fitting preprocessor on training data...\n",
      "Applying UNDERSAMPLE...\n",
      "Class distribution BEFORE balancing:\n",
      "{0: 0.9983, 1: 0.0017}\n",
      "Class distribution AFTER balancing:\n",
      "{0: 0.5, 1: 0.5}\n",
      "✅ Ready for modeling! Train Shape: (756, 30)\n"
     ]
    }
   ],
   "source": [
    "X_cc = cc_df.drop('Class', axis=1)\n",
    "y_cc = cc_df['Class']\n",
    "X_train_bal, y_train_bal, X_test_proc, y_test, preprocessor = prepare_data_for_modeling(\n",
    "    X_cc, y_cc,\n",
    "    dataset_name=\"CreditCard\",\n",
    "    imbalance_technique=\"undersample\",  # or \"smotetomek\"\n",
    "    test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c032d31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
